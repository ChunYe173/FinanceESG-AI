{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1tsVT8o74Jw"
      },
      "source": [
        "# GW -Bert\n",
        "#### training time ~25 hrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIGi8z5aCt29"
      },
      "source": [
        "Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uQCfRzXB72nT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "import argparse\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report, f1_score\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import BigBirdTokenizer, BigBirdForSequenceClassification, BigBirdConfig\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "H_AeA_3ZECBi"
      },
      "outputs": [],
      "source": [
        "# Data Helpers function\n",
        "def get_dataset_splits():\n",
        "    train_fn = \"/content/train.jsonl\"\n",
        "    dev_fn = \"/content/dev.jsonl\"\n",
        "    test_fn = \"/content/test.jsonl\"\n",
        "\n",
        "    def load_data(file_path):\n",
        "        with open(file_path) as f:\n",
        "            data = [json.loads(line) for line in f]\n",
        "            X = [item[\"text\"] for item in data]\n",
        "            y = [item[\"label\"] for item in data]\n",
        "        return X, y\n",
        "\n",
        "    X_train, y_train = load_data(train_fn)\n",
        "    X_validation, y_validation = load_data(dev_fn)\n",
        "    X_test, y_test = load_data(test_fn)\n",
        "\n",
        "    return X_train, y_train, X_validation, y_validation, X_test, y_test\n",
        "\n",
        "def get_cv_splits():\n",
        "    X_train, y_train, X_validation, y_validation, X_test, y_test = get_dataset_splits()\n",
        "    X, y = np.array(X_train + X_validation + X_test), np.array(y_train + y_validation + y_test)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "    skf.get_n_splits(X, y)\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "        X_train, y_train = X[train_index], y[train_index]\n",
        "        X_test, y_test = X[test_index], y[test_index]\n",
        "        yield X_train, y_train, X_test, y_test\n",
        "\n",
        "def round_float(number):\n",
        "    return str(round(number, 3) * 100)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IdQd_xo6DBM6"
      },
      "outputs": [],
      "source": [
        "class SequenceClassificationDataset(Dataset):\n",
        "    def __init__(self, x, tokenizer):\n",
        "        self.examples = x\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        model_inputs = self.tokenizer(\n",
        "            [i for i in batch],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(self.device)\n",
        "        return {\"model_inputs\": model_inputs}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1x-b2CDXDQrh"
      },
      "outputs": [],
      "source": [
        "def evaluate(gold, predictions):\n",
        "    # Calculate precision, recall, F1 score, and accuracy\n",
        "    pr = round_float(metrics.precision_score(gold, predictions))\n",
        "    rc = round_float(metrics.recall_score(gold, predictions))\n",
        "    f1 = round_float(metrics.f1_score(gold, predictions))\n",
        "    acc = round_float(metrics.accuracy_score(gold, predictions))\n",
        "\n",
        "    # Format the results as a string separated by \"&\"\n",
        "    return \" & \".join((pr, rc, f1, acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ePpRwkGkEYwV"
      },
      "outputs": [],
      "source": [
        "class SequenceClassificationDataset(Dataset):\n",
        "    def __init__(self, x, y, tokenizer):\n",
        "        # Combine input sequences (x) and labels (y) into a list of tuples\n",
        "        self.examples = list(zip(x, y))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of examples in the dataset\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return an example (a tuple of sequence and label) based on the index (idx)\n",
        "        return self.examples[idx]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        # Tokenize sequences and create tensors for model inputs and labels\n",
        "        model_inputs = self.tokenizer([i[0] for i in batch], return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
        "        labels = torch.tensor([i[1] for i in batch]).to(self.device)\n",
        "\n",
        "        # Return a dictionary containing model inputs and labels\n",
        "        return {\"model_inputs\": model_inputs, \"label\": labels}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ejUmz0L5EcK6"
      },
      "outputs": [],
      "source": [
        "def evaluate_epoch(model, dataset):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Lists to store true labels, predicted labels, and class probabilities\n",
        "    targets = []\n",
        "    outputs = []\n",
        "    probs = []\n",
        "\n",
        "    # Disable gradient computation during evaluation\n",
        "    with torch.no_grad():\n",
        "        # Iterate through batches in the DataLoader\n",
        "        for batch in DataLoader(dataset, batch_size=args.batch_size, collate_fn=dataset.collate_fn):\n",
        "            # Forward pass to get model predictions\n",
        "            output = model(**batch[\"model_inputs\"])\n",
        "            logits = output.logits\n",
        "\n",
        "            # Extend lists with true labels, predicted labels, and class probabilities\n",
        "            targets.extend(batch['label'].float().tolist())\n",
        "            outputs.extend(logits.argmax(dim=1).tolist())\n",
        "            probs.extend(logits.softmax(dim=1)[:, 1].tolist())\n",
        "\n",
        "    # Return true labels, predicted labels, and class probabilities\n",
        "    return targets, outputs, probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vGyYpXC6Eyq_"
      },
      "outputs": [],
      "source": [
        "def train_model(trainset, model_name):\n",
        "    # Set device (cuda or cpu)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Load configuration for the specified pre-trained model\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    config.num_labels = 2  # Assuming it's a binary classification task\n",
        "    config.gradient_checkpointing = True  # Enables gradient checkpointing for memory efficiency\n",
        "\n",
        "    # Load or initialize the pre-trained sequence classification model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, config=config, cache_dir=\"../../transformer_models/\").to(device)\n",
        "\n",
        "    # Set up training parameters\n",
        "    warmup_steps = 0\n",
        "    train_dataloader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True, collate_fn=trainset.collate_fn)\n",
        "    t_total = int(len(train_dataloader) * args.num_epochs / args.gradient_accumulation_steps)\n",
        "\n",
        "    # Set up optimizer and scheduler\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "    # Zero gradients and set up gradient scaler\n",
        "    model.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "    use_amp = True\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(args.num_epochs):\n",
        "        model.train()\n",
        "        t = tqdm(train_dataloader)\n",
        "        for i, batch in enumerate(t):\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                output = model(**batch[\"model_inputs\"], labels=batch['label'])\n",
        "                loss = output.loss / args.gradient_accumulation_steps\n",
        "\n",
        "            # Backward pass\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % args.gradient_accumulation_steps == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nkt6TaeE_-U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Assuming you have defined or imported the necessary functions and classes like get_dataset_splits,\n",
        "# SequenceClassificationDataset, train_model, evaluate_epoch, and evaluate.\n",
        "\n",
        "def main(args):\n",
        "    model_name = args.model_name\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    except:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    out_str = os.path.basename(model_name) + \" & \"\n",
        "\n",
        "    # Load or define necessary functions and classes like get_dataset_splits, SequenceClassificationDataset, train_model, evaluate_epoch, and evaluate.\n",
        "    X_train, y_train, X_validation, y_validation, X_test, y_test = get_dataset_splits()\n",
        "    trainset = SequenceClassificationDataset(X_train, y_train, tokenizer)\n",
        "    devset = SequenceClassificationDataset(X_validation, y_validation, tokenizer)\n",
        "    model = train_model(trainset, model_name)\n",
        "\n",
        "    # Evaluate dev set\n",
        "    targets, outputs, probs = evaluate_epoch(model, devset)\n",
        "    out_str += evaluate(targets, outputs) + \" & \"\n",
        "\n",
        "    # Evaluate test set\n",
        "    devset = SequenceClassificationDataset(X_test, y_test, tokenizer)\n",
        "    targets, outputs, probs = evaluate_epoch(model, devset)\n",
        "\n",
        "    out_str += evaluate(targets, outputs) + r\" \\\\ \"\n",
        "\n",
        "    print(out_str)\n",
        "    return model, tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming you have already mounted Google Drive\n",
        "    # If not, you can do it using:\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--save_path', type=str, default=\"/content/drive/MyDrive/envclaim-distilroberta\", help='Folder to save the weights')\n",
        "    # ... (other arguments)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    model, tokenizer = main(args)\n",
        "\n",
        "    if args.do_save:\n",
        "        # Save to Google Drive\n",
        "        model.save_pretrained(args.save_path)\n",
        "        tokenizer.save_pretrained(args.save_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6DsP3Low4mL"
      },
      "source": [
        "# Env-Claims\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L-kToXdw4mP"
      },
      "source": [
        "### Installing dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4aWQPPDIw4mQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import sem\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-WaFRnv20RG2"
      },
      "outputs": [],
      "source": [
        "# Data Helpers function\n",
        "def get_dataset_splits():\n",
        "    train_fn = \"/content/train.jsonl\"\n",
        "    dev_fn = \"/content/dev.jsonl\"\n",
        "    test_fn = \"/content/test.jsonl\"\n",
        "\n",
        "    def load_data(file_path):\n",
        "        with open(file_path) as f:\n",
        "            data = [json.loads(line) for line in f]\n",
        "            X = [item[\"text\"] for item in data]\n",
        "            y = [item[\"label\"] for item in data]\n",
        "        return X, y\n",
        "\n",
        "    X_train, y_train = load_data(train_fn)\n",
        "    X_validation, y_validation = load_data(dev_fn)\n",
        "    X_test, y_test = load_data(test_fn)\n",
        "\n",
        "    return X_train, y_train, X_validation, y_validation, X_test, y_test\n",
        "\n",
        "def get_cv_splits():\n",
        "    X_train, y_train, X_validation, y_validation, X_test, y_test = get_dataset_splits()\n",
        "    X, y = np.array(X_train + X_validation + X_test), np.array(y_train + y_validation + y_test)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "    skf.get_n_splits(X, y)\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "        X_train, y_train = X[train_index], y[train_index]\n",
        "        X_test, y_test = X[test_index], y[test_index]\n",
        "        yield X_train, y_train, X_test, y_test\n",
        "\n",
        "def round_float(number):\n",
        "    return str(round(number, 3) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apVeAmEK3obY"
      },
      "source": [
        "* Created a load_data function to handle the common pattern of loading data from a file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkeEifjMw4mR"
      },
      "source": [
        "## SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zfY2FjVRzzWS"
      },
      "outputs": [],
      "source": [
        "#setting Train, Validations splits\n",
        "\n",
        "X_train, y_train, X_validation, y_validation, X_test, y_test = get_dataset_splits()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KEMg5oBw2f0H"
      },
      "outputs": [],
      "source": [
        "def evaluate(gold, predictions):\n",
        "    # Calculate and round evaluation metrics\n",
        "    pr = round_float(metrics.precision_score(gold, predictions))\n",
        "    rc = round_float(metrics.recall_score(gold, predictions))\n",
        "    f1 = round_float(metrics.f1_score(gold, predictions))\n",
        "    acc = round_float(metrics.accuracy_score(gold, predictions))\n",
        "\n",
        "    # Return metrics as a formatted string\n",
        "    return \" & \".join((pr, rc, f1, acc))\n",
        "\n",
        "\n",
        "def evaluate_all(gold, preds):\n",
        "    # Calculate precision, recall, F1 score, and accuracy for each prediction\n",
        "    pr = [metrics.precision_score(g, p) for g, p in zip(gold, preds)]\n",
        "    rc = [metrics.recall_score(g, p) for g, p in zip(gold, preds)]\n",
        "    f1 = [metrics.f1_score(g, p) for g, p in zip(gold, preds)]\n",
        "    acc = [metrics.accuracy_score(g, p) for g, p in zip(gold, preds)]\n",
        "\n",
        "    # Calculate mean and standard error for each metric\n",
        "    mean_pr, sem_pr = np.mean(pr), sem(pr).round(3) * 100\n",
        "    mean_rc, sem_rc = np.mean(rc), sem(rc).round(3) * 100\n",
        "    mean_f1, sem_f1 = np.mean(f1), sem(f1).round(3) * 100\n",
        "    mean_acc, sem_acc = np.mean(acc), sem(acc).round(3) * 100\n",
        "\n",
        "    # Format the output string\n",
        "    out_str = (\n",
        "        f\"{round_float(mean_pr)} \\\\pm {sem_pr} & \"\n",
        "        f\"{round_float(mean_rc)} \\\\pm {sem_rc} & \"\n",
        "        f\"{round_float(mean_f1)} \\\\pm {sem_f1} & \"\n",
        "        f\"{round_float(mean_acc)} \\\\pm {sem_acc}\"\n",
        "    )\n",
        "\n",
        "    return out_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5ZDmc6p4GGM",
        "outputId": "f4b62eb6-3a1f-4698-a0e4-dbd478547513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train & 2117 & 24.5 & 0.25 \\\\ \\hline\n",
            "dev & 265 & 24.4 & 0.25 \\\\ \\hline\n",
            "test & 265 & 24.2 & 0.25 \\\\ \\hline\n",
            "all & 2647 & 24.5 & 0.25 \\\\ \\hline\n"
          ]
        }
      ],
      "source": [
        "def create_table_1():\n",
        "    X_train, y_train, X_validation, y_validation, X_test, y_test = get_dataset_splits()\n",
        "\n",
        "    for split in [\"train\", \"dev\", \"test\", \"all\"]:\n",
        "        out_str = f\"{split} & \"\n",
        "\n",
        "        if split == \"train\":\n",
        "            out_str += f\"{len(X_train)} & \"\n",
        "            out_str += f\"{np.mean([len(i.split()) for i in X_train]).round(1)} & \"\n",
        "            out_str += f\"{np.mean(y_train).round(2)}\"\n",
        "\n",
        "        elif split == \"dev\":\n",
        "            out_str += f\"{len(X_validation)} & \"\n",
        "            out_str += f\"{np.mean([len(i.split()) for i in X_validation]).round(1)} & \"\n",
        "            out_str += f\"{np.mean(y_validation).round(2)}\"\n",
        "\n",
        "        elif split == \"test\":\n",
        "            out_str += f\"{len(X_test)} & \"\n",
        "            out_str += f\"{np.mean([len(i.split()) for i in X_test]).round(1)} & \"\n",
        "            out_str += f\"{np.mean(y_test).round(2)}\"\n",
        "\n",
        "        elif split == \"all\":\n",
        "            X_all = X_train + X_validation + X_test\n",
        "            y_all = y_train + y_validation + y_test\n",
        "            out_str += f\"{len(X_all)} & \"\n",
        "            out_str += f\"{np.mean([len(i.split()) for i in X_all]).round(1)} & \"\n",
        "            out_str += f\"{np.mean(y_all).round(2)}\"\n",
        "\n",
        "        out_str += r\" \\\\ \\hline\"\n",
        "        print(out_str)\n",
        "\n",
        "create_table_1()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcjvEV-o2mZh",
        "outputId": "012b0280-c626-49b6-cdcd-fdc966940361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "majority & 0.0 & 0.0 & 0.0 & 74.9 & 0.0 & 0.0 & 0.0 & 75.1 & 0.0 & 0.0 & 0.0 & 74.7 \\\\\n",
            "random & 26.700000000000003 & 53.1 & 35.5 & 51.6 & 23.599999999999998 & 43.9 & 30.7 & 50.6 & 26.400000000000002 & 52.6 & 35.199999999999996 & 51.2 \\\\\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def baselines():\n",
        "    # Majority\n",
        "    out_str = \"majority & \"\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    # Cross-validation\n",
        "    for X_train, y_train, X_test, y_test in get_cv_splits():\n",
        "        all_labels.extend(y_test)\n",
        "        preds = [0] * len(y_test)\n",
        "        all_preds.extend(preds)\n",
        "\n",
        "    out_str += evaluate(all_labels, all_preds) + \" & \"\n",
        "\n",
        "    # Dev set\n",
        "    X_train, y_train, X_validation, y_validation, X_test, y_test = get_dataset_splits()\n",
        "    preds = [0] * len(y_validation)\n",
        "    out_str += evaluate(y_validation, preds) + \" & \"\n",
        "\n",
        "    # Test set\n",
        "    preds = [0] * len(y_test)\n",
        "    out_str += evaluate(y_test, preds) + r\" \\\\\"\n",
        "\n",
        "    print(out_str)\n",
        "\n",
        "    # Random\n",
        "    out_str = \"random & \"\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for X_train, y_train, X_test, y_test in get_cv_splits():\n",
        "        all_labels.extend(y_test)\n",
        "        preds = np.random.randint(0, 2, size=len(y_test), dtype=int)\n",
        "        all_preds.extend(preds)\n",
        "\n",
        "    out_str += evaluate(all_labels, all_preds) + \" & \"\n",
        "\n",
        "    # Dev set\n",
        "    preds = np.random.randint(0, 2, size=len(y_validation), dtype=int)\n",
        "    out_str += evaluate(y_validation, preds) + \" & \"\n",
        "\n",
        "    # Test set\n",
        "    preds = np.random.randint(0, 2, size=len(y_test), dtype=int)\n",
        "    out_str += evaluate(y_test, preds) + r\" \\\\\"\n",
        "\n",
        "    print(out_str)\n",
        "\n",
        "baselines()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEqsQx0w2m3I",
        "outputId": "70d74722-8c60-477e-d188-af88e712a04a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF SVM & 71.1 & 65.9 & 68.4 & 84.7 & 67.7 & 63.6 & 65.60000000000001 & 83.39999999999999 & 68.10000000000001 & 70.1 & 69.1 & 84.2 & \n"
          ]
        }
      ],
      "source": [
        "def tf_idf_baseline():\n",
        "    classifier = LinearSVC(max_iter=50000)\n",
        "    parameters = {\n",
        "        'vect__max_features': [10000, 20000, 40000],\n",
        "        'clf__C': [0.1, 1, 10],\n",
        "        'clf__loss': ('hinge', 'squared_hinge')\n",
        "    }\n",
        "\n",
        "    out_str = \"TF-IDF SVM & \"\n",
        "\n",
        "    # Cross-validation\n",
        "    all_preds, all_labels = [], []\n",
        "    for X_train, y_train, X_test, y_test in get_cv_splits():\n",
        "        text_clf = Pipeline([\n",
        "            ('vect', CountVectorizer(stop_words='english', ngram_range=(1, 3), min_df=5)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', classifier),\n",
        "        ])\n",
        "        text_clf.fit(X_train, y_train)\n",
        "        all_preds.extend(text_clf.predict(X_test))\n",
        "        all_labels.extend(y_test)\n",
        "\n",
        "    out_str += evaluate(all_labels, all_preds) + \" & \"\n",
        "\n",
        "    # Dev set and test set\n",
        "    X_train, y_train, X_validation, y_validation, X_test, y_test = get_dataset_splits()\n",
        "\n",
        "    for split, X, y in zip([\"dev\", \"test\"], [X_validation, X_test], [y_validation, y_test]):\n",
        "        text_clf = Pipeline([\n",
        "            ('vect', CountVectorizer(stop_words='english', ngram_range=(1, 3), min_df=5)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', classifier),\n",
        "        ])\n",
        "        text_clf.fit(X_train, y_train)\n",
        "        preds = text_clf.predict(X)\n",
        "        out_str += evaluate(y, preds) + \" & \"\n",
        "\n",
        "    print(out_str)\n",
        "\n",
        "tf_idf_baseline()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5BcwY8g20qg",
        "outputId": "b0982404-af32-43c8-b5d6-a36c5ca806a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:558: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:558: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:558: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:558: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:558: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:558: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:558: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Character n-gram SVM & 76.8 & 63.6 & 69.6 & 86.0 & 69.19999999999999 & 68.2 & 68.7 & 84.5 & 75.0 & 67.2 & 70.89999999999999 & 86.0 & \n"
          ]
        }
      ],
      "source": [
        "def character_n_gram_baseline():\n",
        "    classifier = LinearSVC(max_iter=50000)\n",
        "    parameters = {\n",
        "        'vect__max_features': [10000, 20000, 40000],\n",
        "        'clf__C': [0.1, 1, 10],\n",
        "        'clf__loss': ('hinge', 'squared_hinge')\n",
        "    }\n",
        "\n",
        "    out_str = \"Character n-gram SVM & \"\n",
        "\n",
        "    # Cross-validation\n",
        "    all_preds, all_labels = [], []\n",
        "    for X_train, y_train, X_test, y_test in get_cv_splits():\n",
        "        text_clf = Pipeline([\n",
        "            ('vect', CountVectorizer(ngram_range=(1, 10), token_pattern=r\"(?u)\\b\\w+\\b\", analyzer='char', min_df=5)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', classifier),\n",
        "        ])\n",
        "        text_clf.fit(X_train, y_train)\n",
        "        all_preds.extend(text_clf.predict(X_test))\n",
        "        all_labels.extend(y_test)\n",
        "\n",
        "    out_str += evaluate(all_labels, all_preds) + \" & \"\n",
        "\n",
        "    # Dev set and test set\n",
        "    X_train, y_train, X_validation, y_validation, X_test, y_test = get_dataset_splits()\n",
        "\n",
        "    for split, X, y in zip([\"dev\", \"test\"], [X_validation, X_test], [y_validation, y_test]):\n",
        "        text_clf = Pipeline([\n",
        "            ('vect', CountVectorizer(ngram_range=(1, 10), token_pattern=r\"(?u)\\b\\w+\\b\", analyzer='char', min_df=10)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', classifier),\n",
        "        ])\n",
        "        text_clf.fit(X_train, y_train)\n",
        "        preds = text_clf.predict(X)\n",
        "        out_str += evaluate(y, preds) + \" & \"\n",
        "\n",
        "    print(out_str)\n",
        "\n",
        "character_n_gram_baseline()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
